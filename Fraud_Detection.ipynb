{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#import tensorflow as tf\n",
    "\n",
    "from pylab import rcParams\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "LABELS = [\"Normal\", \"Fraud\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data\n",
    "\n",
    "The dataset we're going to use can be downloaded from [Kaggle](https://www.kaggle.com/dalpozz/creditcardfraud). It contains data about credit card transactions that occurred during a period of two days, with 492 frauds out of 284,807 transactions.\n",
    "\n",
    "All variables in the dataset are numerical. The data has been transformed using PCA transformation(s) due to privacy reasons. The two features that haven't been changed are Time and Amount. Time contains the seconds elapsed between each transaction and the first transaction in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "31 columns, 2 of which are Time and Amount. The rest are output from the PCA transformation. Let's check for missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEWCAYAAACwtjr+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIqr7//xLIMVIVQF2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18d87d95518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count_classes = pd.value_counts(df['Class'], sort = True)\n",
    "count_classes.plot(kind = 'bar', rot=0)\n",
    "plt.title(\"Transaction class distribution\")\n",
    "plt.xticks(range(2), LABELS)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data\n",
    "\n",
    "First, let's drop the Time column (not going to use it) and use the scikit's StandardScaler on the Amount. The scaler removes the mean and scales the values to unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = df.drop(['Time'], axis=1)\n",
    "\n",
    "data['Amount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training our Autoencoder is gonna be a bit different from what we are used to. Let's say you have a dataset containing a lot of non fraudulent transactions at hand. You want to detect any anomaly on new transactions. We will create this situation by training our model on the normal transactions, only. Reserving the correct class on the test set will give us a way to evaluate the performance of our model. We will reserve 20% of our data for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = np.array(data)\n",
    "\n",
    "X_train, X_test = train_test_split(data, test_size=0.2, random_state=RANDOM_SEED)\n",
    "X_train = X_train[X_train[:,-1] == 0]\n",
    "X_train = X_train[:,:-1]\n",
    "\n",
    "y_test = X_test[:,-1]\n",
    "X_test = X_test[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227451, 29)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model\n",
    "\n",
    "Building an autoencoder with 100 hidden layer neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder = Sequential()\n",
    "\n",
    "autoencoder.add(Dropout(0.2, input_shape=(input_dim,)))\n",
    "autoencoder.add(Dense(20, activation='sigmoid'))\n",
    "autoencoder.add(Dense(encoding_dim, activation='relu'))\n",
    "autoencoder.add(Dense(20, activation='sigmoid'))\n",
    "autoencoder.add(Dense(input_dim, activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our model for 100 epochs with a batch size of 32 samples and save the best performing model to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_epoch = 100\n",
    "batch_size = 32\n",
    "\n",
    "autoencoder.compile(optimizer='adam', \n",
    "                    loss='mean_squared_error', \n",
    "                    metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.6266 - mean_squared_error: 0.6266 - val_loss: 0.4506 - val_mean_squared_error: 0.4506\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 8s - loss: 0.4634 - mean_squared_error: 0.4634 - val_loss: 0.3698 - val_mean_squared_error: 0.3698\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 9s - loss: 0.4135 - mean_squared_error: 0.4135 - val_loss: 0.3312 - val_mean_squared_error: 0.3312\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 9s - loss: 0.3891 - mean_squared_error: 0.3891 - val_loss: 0.3133 - val_mean_squared_error: 0.3133\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 12s - loss: 0.3723 - mean_squared_error: 0.3723 - val_loss: 0.3007 - val_mean_squared_error: 0.3007\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 14s - loss: 0.3592 - mean_squared_error: 0.3592 - val_loss: 0.2907 - val_mean_squared_error: 0.2907\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 13s - loss: 0.3514 - mean_squared_error: 0.3514 - val_loss: 0.2858 - val_mean_squared_error: 0.2858\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.3481 - mean_squared_error: 0.3481 - val_loss: 0.2832 - val_mean_squared_error: 0.2832\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 9s - loss: 0.3437 - mean_squared_error: 0.3437 - val_loss: 0.2806 - val_mean_squared_error: 0.2806\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 9s - loss: 0.3379 - mean_squared_error: 0.3379 - val_loss: 0.2736 - val_mean_squared_error: 0.2736\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.3346 - mean_squared_error: 0.3346 - val_loss: 0.2712 - val_mean_squared_error: 0.2712\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 9s - loss: 0.3287 - mean_squared_error: 0.3287 - val_loss: 0.2653 - val_mean_squared_error: 0.2653\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.3268 - mean_squared_error: 0.3268 - val_loss: 0.2612 - val_mean_squared_error: 0.2612\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 9s - loss: 0.3223 - mean_squared_error: 0.3223 - val_loss: 0.2593 - val_mean_squared_error: 0.2593\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 9s - loss: 0.3209 - mean_squared_error: 0.3209 - val_loss: 0.2608 - val_mean_squared_error: 0.2608\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 8s - loss: 0.3172 - mean_squared_error: 0.3172 - val_loss: 0.2597 - val_mean_squared_error: 0.2597\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 8s - loss: 0.3149 - mean_squared_error: 0.3149 - val_loss: 0.2579 - val_mean_squared_error: 0.2579\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 8s - loss: 0.3153 - mean_squared_error: 0.3153 - val_loss: 0.2535 - val_mean_squared_error: 0.2535\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 9s - loss: 0.3109 - mean_squared_error: 0.3109 - val_loss: 0.2515 - val_mean_squared_error: 0.2515\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 8s - loss: 0.3094 - mean_squared_error: 0.3094 - val_loss: 0.2502 - val_mean_squared_error: 0.2502\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 9s - loss: 0.3080 - mean_squared_error: 0.3080 - val_loss: 0.2486 - val_mean_squared_error: 0.2486\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 8s - loss: 0.3048 - mean_squared_error: 0.3048 - val_loss: 0.2471 - val_mean_squared_error: 0.2471\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 9s - loss: 0.3015 - mean_squared_error: 0.3015 - val_loss: 0.2417 - val_mean_squared_error: 0.2417\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 9s - loss: 0.3007 - mean_squared_error: 0.3007 - val_loss: 0.2435 - val_mean_squared_error: 0.2435\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 8s - loss: 0.2976 - mean_squared_error: 0.2976 - val_loss: 0.2393 - val_mean_squared_error: 0.2393\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 9s - loss: 0.2972 - mean_squared_error: 0.2972 - val_loss: 0.2410 - val_mean_squared_error: 0.2410\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 8s - loss: 0.2981 - mean_squared_error: 0.2981 - val_loss: 0.2452 - val_mean_squared_error: 0.2452\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 8s - loss: 0.2969 - mean_squared_error: 0.2969 - val_loss: 0.2402 - val_mean_squared_error: 0.2402\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 8s - loss: 0.2946 - mean_squared_error: 0.2946 - val_loss: 0.2371 - val_mean_squared_error: 0.2371\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 8s - loss: 0.2965 - mean_squared_error: 0.2965 - val_loss: 0.2345 - val_mean_squared_error: 0.2345\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 8s - loss: 0.2928 - mean_squared_error: 0.2928 - val_loss: 0.2356 - val_mean_squared_error: 0.2356\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 9s - loss: 0.2925 - mean_squared_error: 0.2925 - val_loss: 0.2380 - val_mean_squared_error: 0.2380\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 9s - loss: 0.2919 - mean_squared_error: 0.2919 - val_loss: 0.2350 - val_mean_squared_error: 0.2350\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 8s - loss: 0.2922 - mean_squared_error: 0.2922 - val_loss: 0.2375 - val_mean_squared_error: 0.2375\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 8s - loss: 0.2915 - mean_squared_error: 0.2915 - val_loss: 0.2334 - val_mean_squared_error: 0.2334\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 9s - loss: 0.2920 - mean_squared_error: 0.2920 - val_loss: 0.2333 - val_mean_squared_error: 0.2333\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 9s - loss: 0.2905 - mean_squared_error: 0.2905 - val_loss: 0.2300 - val_mean_squared_error: 0.2300\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 9s - loss: 0.2897 - mean_squared_error: 0.2897 - val_loss: 0.2323 - val_mean_squared_error: 0.2323\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 9s - loss: 0.2903 - mean_squared_error: 0.2903 - val_loss: 0.2325 - val_mean_squared_error: 0.2325\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227451/227451 [==============================] - 9s - loss: 0.2893 - mean_squared_error: 0.2893 - val_loss: 0.2316 - val_mean_squared_error: 0.2316\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 11s - loss: 0.2917 - mean_squared_error: 0.2917 - val_loss: 0.2332 - val_mean_squared_error: 0.2332\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 14s - loss: 0.2891 - mean_squared_error: 0.2891 - val_loss: 0.2326 - val_mean_squared_error: 0.2326\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 11s - loss: 0.2891 - mean_squared_error: 0.2891 - val_loss: 0.2316 - val_mean_squared_error: 0.2316\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2880 - mean_squared_error: 0.2880 - val_loss: 0.2329 - val_mean_squared_error: 0.2329\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2883 - mean_squared_error: 0.2883 - val_loss: 0.2304 - val_mean_squared_error: 0.2304\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2876 - mean_squared_error: 0.2876 - val_loss: 0.2308 - val_mean_squared_error: 0.2308\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 11s - loss: 0.2900 - mean_squared_error: 0.2900 - val_loss: 0.2361 - val_mean_squared_error: 0.2361\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2864 - mean_squared_error: 0.2864 - val_loss: 0.2299 - val_mean_squared_error: 0.2299\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 9s - loss: 0.2860 - mean_squared_error: 0.2860 - val_loss: 0.2412 - val_mean_squared_error: 0.2412\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2878 - mean_squared_error: 0.2878 - val_loss: 0.2308 - val_mean_squared_error: 0.2308\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 9s - loss: 0.2856 - mean_squared_error: 0.2856 - val_loss: 0.2343 - val_mean_squared_error: 0.2343\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2872 - mean_squared_error: 0.2872 - val_loss: 0.2268 - val_mean_squared_error: 0.2268\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2859 - mean_squared_error: 0.2859 - val_loss: 0.2281 - val_mean_squared_error: 0.2281\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2854 - mean_squared_error: 0.2854 - val_loss: 0.2272 - val_mean_squared_error: 0.2272\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2849 - mean_squared_error: 0.2849 - val_loss: 0.2321 - val_mean_squared_error: 0.2321\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2851 - mean_squared_error: 0.2851 - val_loss: 0.2305 - val_mean_squared_error: 0.2305\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2861 - mean_squared_error: 0.2861 - val_loss: 0.2332 - val_mean_squared_error: 0.2332\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2859 - mean_squared_error: 0.2859 - val_loss: 0.2330 - val_mean_squared_error: 0.2330\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2850 - mean_squared_error: 0.2850 - val_loss: 0.2300 - val_mean_squared_error: 0.2300\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2845 - mean_squared_error: 0.2845 - val_loss: 0.2327 - val_mean_squared_error: 0.2327\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2845 - mean_squared_error: 0.2845 - val_loss: 0.2301 - val_mean_squared_error: 0.2301\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2835 - mean_squared_error: 0.2835 - val_loss: 0.2277 - val_mean_squared_error: 0.2277\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2849 - mean_squared_error: 0.2849 - val_loss: 0.2303 - val_mean_squared_error: 0.2303\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2843 - mean_squared_error: 0.2843 - val_loss: 0.2280 - val_mean_squared_error: 0.2280\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2839 - mean_squared_error: 0.2839 - val_loss: 0.2255 - val_mean_squared_error: 0.2255\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2846 - mean_squared_error: 0.2846 - val_loss: 0.2289 - val_mean_squared_error: 0.2289\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2845 - mean_squared_error: 0.2845 - val_loss: 0.2274 - val_mean_squared_error: 0.2274\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2846 - mean_squared_error: 0.2846 - val_loss: 0.2320 - val_mean_squared_error: 0.2320\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2823 - mean_squared_error: 0.2823 - val_loss: 0.2308 - val_mean_squared_error: 0.2308\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2829 - mean_squared_error: 0.2829 - val_loss: 0.2257 - val_mean_squared_error: 0.2257\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 11s - loss: 0.2833 - mean_squared_error: 0.2833 - val_loss: 0.2291 - val_mean_squared_error: 0.2291\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2844 - mean_squared_error: 0.2844 - val_loss: 0.2285 - val_mean_squared_error: 0.2285\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2806 - mean_squared_error: 0.2806 - val_loss: 0.2274 - val_mean_squared_error: 0.2274\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 11s - loss: 0.2832 - mean_squared_error: 0.2832 - val_loss: 0.2331 - val_mean_squared_error: 0.2331\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2824 - mean_squared_error: 0.2824 - val_loss: 0.2284 - val_mean_squared_error: 0.2284\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2829 - mean_squared_error: 0.2829 - val_loss: 0.2293 - val_mean_squared_error: 0.2293\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2813 - mean_squared_error: 0.2813 - val_loss: 0.2241 - val_mean_squared_error: 0.2241\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2834 - mean_squared_error: 0.2834 - val_loss: 0.2295 - val_mean_squared_error: 0.2295\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227451/227451 [==============================] - 10s - loss: 0.2828 - mean_squared_error: 0.2828 - val_loss: 0.2280 - val_mean_squared_error: 0.2280\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2806 - mean_squared_error: 0.2806 - val_loss: 0.2307 - val_mean_squared_error: 0.2307\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 9s - loss: 0.2824 - mean_squared_error: 0.2824 - val_loss: 0.2304 - val_mean_squared_error: 0.2304\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 9s - loss: 0.2810 - mean_squared_error: 0.2810 - val_loss: 0.2263 - val_mean_squared_error: 0.2263\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 12s - loss: 0.2805 - mean_squared_error: 0.2805 - val_loss: 0.2296 - val_mean_squared_error: 0.2296\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2815 - mean_squared_error: 0.2815 - val_loss: 0.2367 - val_mean_squared_error: 0.2367\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2796 - mean_squared_error: 0.2796 - val_loss: 0.2260 - val_mean_squared_error: 0.2260\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2809 - mean_squared_error: 0.2809 - val_loss: 0.2234 - val_mean_squared_error: 0.2234\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2800 - mean_squared_error: 0.2800 - val_loss: 0.2236 - val_mean_squared_error: 0.2236\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 11s - loss: 0.2768 - mean_squared_error: 0.2768 - val_loss: 0.2258 - val_mean_squared_error: 0.2258\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2773 - mean_squared_error: 0.2773 - val_loss: 0.2242 - val_mean_squared_error: 0.2242\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2795 - mean_squared_error: 0.2795 - val_loss: 0.2233 - val_mean_squared_error: 0.2233\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2778 - mean_squared_error: 0.2778 - val_loss: 0.2241 - val_mean_squared_error: 0.2241\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2782 - mean_squared_error: 0.2782 - val_loss: 0.2259 - val_mean_squared_error: 0.2259\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 11s - loss: 0.2786 - mean_squared_error: 0.2786 - val_loss: 0.2291 - val_mean_squared_error: 0.2291\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 9s - loss: 0.2767 - mean_squared_error: 0.2767 - val_loss: 0.2263 - val_mean_squared_error: 0.2263\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 9s - loss: 0.2785 - mean_squared_error: 0.2785 - val_loss: 0.2247 - val_mean_squared_error: 0.2247\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 10s - loss: 0.2761 - mean_squared_error: 0.2761 - val_loss: 0.2228 - val_mean_squared_error: 0.2228\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 9s - loss: 0.2759 - mean_squared_error: 0.2759 - val_loss: 0.2252 - val_mean_squared_error: 0.2252\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 9s - loss: 0.2777 - mean_squared_error: 0.2777 - val_loss: 0.2269 - val_mean_squared_error: 0.2269\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 9s - loss: 0.2791 - mean_squared_error: 0.2791 - val_loss: 0.2289 - val_mean_squared_error: 0.2289\n",
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/1\n",
      "227451/227451 [==============================] - 9s - loss: 0.2773 - mean_squared_error: 0.2773 - val_loss: 0.2253 - val_mean_squared_error: 0.2253\n"
     ]
    }
   ],
   "source": [
    "hist = []\n",
    "for _ in range(100):\n",
    "    hist.append(autoencoder.fit(X_train, X_train,\n",
    "                    epochs=1,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_test, X_test),\n",
    "                    verbose=1).history)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_hist_ = [(x['loss'][0], x['val_loss'][0]) for x in hist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# autoencoder.save('fraud_detection.h5')\n",
    "from keras.models import load_model\n",
    "aec = load_model('fraud_detection.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(list(zip(*_hist_))[0])\n",
    "plt.plot(list(zip(*_hist_))[1])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reconstruction error on our training and test data seems to converge nicely. Is it low enough? Let's have a closer look at the error distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = aec.predict(X_train)\n",
    "recon  = (((predictions-X_train)**2).mean(-1))\n",
    "\n",
    "test_pred = autoencoder.predict(X_test)\n",
    "test_recon  = (((test_pred-X_test)**2).mean(-1))\n",
    "\n",
    "mean_recon = recon.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score,recall_score\n",
    "\n",
    "prc = list()\n",
    "rec = list()\n",
    "thres = list()\n",
    "\n",
    "th = 0\n",
    "for i in range(100):\n",
    "    th+=0.1\n",
    "    fraud = (test_recon>mean_recon+th)\n",
    "    prc.append(precision_score(y_test,fraud))\n",
    "    rec.append(recall_score(y_test,fraud))\n",
    "    thres.append(th)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(prc, rec)\n",
    "plt.title('precision vs recall')\n",
    "plt.ylabel('recall')\n",
    "plt.xlabel('precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(thres, rec)\n",
    "plt.title('threshold vs recall')\n",
    "plt.ylabel('recall')\n",
    "plt.xlabel('threshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(thres, prc)\n",
    "plt.title('threshold vs precision')\n",
    "plt.ylabel('precision')\n",
    "plt.xlabel('threshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
